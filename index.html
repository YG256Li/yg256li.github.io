<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yangguang Li</title>

  <meta name="author" content="Yangguang Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/ut_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">

            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/lyg.png"><img style="width:85%;max-width:85%" alt="profile photo" src="images/lyg.png" class="hoverZoomLink"></a>
            </td>
            
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yangguang Li(李阳光) </name>
              </p>
              <p>
                I am working at <a href="https://www.sensetime.com/cn">SenseTime</a> from 2019 to now. I obtained my master degree from <a href="https://www.bupt.edu.cn">Beijing University of Posts and Telecommunications</a> and bachelor degree from <a href="http://www.tju.edu.cn">Tianjin University</a>.
              </p>
              <p>
                Currently, I am focusing on the exploitation and exploration of autonomous driving camera perception models(Monocular and BEV). Before that I focused on general vision(multi-modal/big model) and Detection(face/human/traffic/structure/keypoint/video) research. 
              </p>
              <p style="text-align:center">
                <a href="mailto:liyangguang@sensetime.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV_JeffLiang.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?user=ecTFCUMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/yg-li-237683223/">Linkedin</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/atom-74">Zhihu</a> &nbsp/&nbsp
                <a href="https://github.com/Sense-GVT">Github</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <!-- <div style="width:100%;overflow-y:scroll; height:230px;"> -->
                <ul>
                  <li style="line-height:30px"> <b>September 2022:</b> One paper gets accepted to NeurIPS 2022.</li>
                  <li style="line-height:30px"> <b>August 2022:</b> One paper gets accepted to COLING 2022.</li>
                  <li style="line-height:30px"> <b>August 2022:</b> <a href="https://github.com/enyac-group/supmae">SupMAE</a> code is available.</li>
                  <li style="line-height:30px"> <b>July 2022:</b> <a href="https://github.com/Sense-GVT/BCDNet">BCDNet</a> code is available.</li>
                  <li style="line-height:30px"> <b>July 2022:</b> One paper gets accepted to ECCV 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> 1st place in Embodied AI Challenge @ CVPR 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> 2nd place in UG2+ Challenge @ CVPR 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> One papers gets accepted to ICML workshops 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> One papers gets accepted to CVPR workshops 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> One paper gets accepted to IJCAI 2022 as <b>long oral</b>.</li>
                  <li style="line-height:30px"> <b>February 2022:</b> <a href="https://github.com/Sense-GVT/SNCSE">SNCSE</a> code is available.</li>
                  <li style="line-height:30px"> <b>January 2022:</b> SenseTime Team Award, SenseTime’s highest award.</li>
                  <li style="line-height:30px"> <b>January 2022:</b> <a href="https://github.com/Sense-GVT/DeCLIP">DeCLIP & DeCLIP benchmark</a> code are available.</li>
                  <li style="line-height:30px"> <b>January 2022:</b> One paper gets accepted to ICLR 2022.</li>
                <ul>
                <!-- </div> -->
		    </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bcdnet.png" alt="bcdnet" width="280" height="100" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2209.01404">
                <papertitle>Towards Accurate Binary Neural Networks via Modeling Contextual Dependencies</papertitle>
              </a>
              <br>
              <a href="">Xingrun Xing</a>,
              <strong>Yangguang Li</strong>,
              <a href="https://weivision.github.io/">Wei Li</a>,
              <a href="">Wenrui Ding</a>,
              <a href="">Yalong Jiang</a>,
              <a href="">Yufeng Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <a href="">Chunlei Liu</a>,
              <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>,
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2209.01404">arxiv</a>,
              <a href="data/bcdnet.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/BCDNet">code</a>
              <p></p>
              <p>We propose BCDNet, a newly designed method to binary neural modules, which enables BNNs to learn effective contextual dependencies.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/declip.png" alt="declip" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=zq1iJkNk3uN">
                <papertitle>Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</papertitle>
              </a>
              <br>
              <strong>Yangguang Li*</strong>,
              <a href="https://jeff-liangf.github.io/">Feng Liang*</a>,
              <a href="https://openreview.net/profile?id=~Lichen_Zhao1">Lichen Zhao*</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=5Ydha2EAAAAJ">Yufeng Cui</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>
              <a href="https://amandajshao.github.io/">Jing Shao</a>,
              <a href="https://forwil.xyz/">Fengwei Yu</a>,
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>
              <br>
              <em>ICLR</em>, 2022
              <br>
              <a href="https://openreview.net/pdf?id=zq1iJkNk3uN">arxiv</a>,
              <a href="data/declip.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/DeCLIP">code</a>,
              <a href="https://recorder-v3.slideslive.com/#/share?share=62378&s=d93b81b1-d9de-42b7-9437-8acc34fbf94e">video presentation</a>
              <p></p>
              <p>We propose Data efficient CLIP (DeCLIP), a method to efficiently train CLIP via utilizing the widespread supervision among the image-text data.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/declip_benchmark.png" alt="declip_benchmark" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=wAEizNH9jJ6">
                <papertitle>Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=5Ydha2EAAAAJ">Yufeng Cui*</a>,
              <a href="https://openreview.net/profile?id=~Lichen_Zhao1">Lichen Zhao*</a>,
              <a href="https://jeff-liangf.github.io/">Feng Liang*</a>,
              <strong>Yangguang Li<sup>&dagger;</sup></strong>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>ICMLW</em>, 2022
              <br>
              <a href="https://openreview.net/pdf?id=wAEizNH9jJ6">arxiv</a>,
              <a href="data/declip_benchmark.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/DeCLIP">code</a>
              <p></p>
              <p>We propose a CLIP benchmark of data, model, and supervision.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/repre.png" alt="repre" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2201.06857">
                <papertitle>RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training</papertitle>
              </a>
              <br>
              <a href="">Luya Wang</a>,
              <a href="https://jeff-liangf.github.io/">Feng Liang</a>,
              <strong>Yangguang Li</strong>,
              <a href="">Honggang Zhang</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>IJCAI</em>, 2022, <b>Long oral</b>
              <br>
              <a href="https://arxiv.org/abs/2201.06857">arxiv</a>,
              <a href="data/repre.bib">bibtex</a>
              <p></p>
              <p>We propose RePre to extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/imci.png" alt="imci" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.14001">
                <papertitle>IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification</papertitle>
              </a>
              <br>
              <a href="">Hao Wang</a>,
              <strong>Yangguang Li</strong>,
              <a href="">Zhen Huang</a>,
              <a href="">Yong Dou</a>,
              <br>
              <em>COLING</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2208.14001">arxiv</a>,
              <a href="data/imci.bib">bibtex</a>
              <p></p>
              <p>We propose RePre to extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Honors</heading>
                <ul>
                  <li style="line-height:30px"> SenseTime Team Award, SenseTime’s highest award, 2021.</li>
                  <li style="line-height:30px"> SenseTime Outstanding Intern, 2020.</li>
                  <li style="line-height:30px"> SenseTime Outstanding Intern, 2019.</li>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
                <p>
                  <li style="line-height:30px"> Reviewer in ICML 2022, ECCV 2022, NeurIPS 2022</li>
                  <li style="line-height:30px"> Organizer in ECCV 2022 Computer Vision in the Wild Challenge</li>
                </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                <a href="https://clustrmaps.com/site/1bqdz"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=XGFQwNh_7Kz54sbp6qL_1YSxklUHJ1TQvZdqIhX0S94&cl=ffffff" /></a>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/Jeff-LiangF/jeff-liangf.github.io">Feng (Jeff) Liang</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>