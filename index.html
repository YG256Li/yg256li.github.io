<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yangguang Li</title>

  <meta name="author" content="Yangguang Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/ut_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">

            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/lyg.png"><img style="width:85%;max-width:85%" alt="profile photo" src="images/lyg.png" class="hoverZoomLink"></a>
            </td>
            
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yangguang Li </name>
              </p>
              <p style="text-align:center">
                I am a Senior Consultant at Shanghai AI Lab from 2021. 
                </p>
              <p style="text-align:center">
              I am a Research Director at VAST from 2023. 
              </p>

              <p style="text-align:center">
              Before that I was a Research Leader at SenseTime</a>. 
              <!-- I obtained my master degree from <a href="https://www.bupt.edu.cn">Beijing University of Posts and Telecommunications</a> and bachelor degree from <a href="http://www.tju.edu.cn">Tianjin University</a>. -->
              </p>

              <p style="text-align:center">
              I am focusing on 3D generation research.
              </p>
              <p style="text-align:center">
                <p style="text-align:center">
                  I am PhD from the Chinese University of Hong Kong.
                  </p>
                  <p style="text-align:center">
                <a href="mailto:liyangguang256@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV_JeffLiang.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?user=ecTFCUMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                <!-- <a href="https://www.linkedin.com/in/yg-li-237683223/">Linkedin</a> &nbsp/&nbsp -->
                <!-- <a href="https://www.zhihu.com/people/atom-74">Zhihu</a> &nbsp/&nbsp -->
                <!-- <a href="https://github.com/Sense-GVT">Github</a> &nbsp/&nbsp -->
		            <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=a7AMvgkAAAAJ">Google Scholar</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News:</heading>
                <ul>
		  <li style="line-height:30px"> <b>2025.05:</b> I will be a speaker at ICCV 2025 tutorial about 3D Diffusion and Auto-Regressive generation models.</li>
		  <li style="line-height:30px"> <b>2025.04:</b> I serve as an Area Chair for NeurIPS 2025.</li>
                  <li style="line-height:30px"> <b>2025.03:</b> We release <a href="https://github.com/VAST-AI-Research/TripoSF">TripoSF</a>, which was the best image-to-3D generation model.</li>
                  <li style="line-height:30px"> <b>2025.02:</b> We release <a href="https://github.com/VAST-AI-Research/TripoSG">TripoSG</a>, which was the best image-to-3D generation model at the time.</li>
                  <li style="line-height:30px"> <b>2024.12:</b> Our paper <a href="https://arxiv.org/abs/2411.14740">TexGen</a> won the <span style="color:red;">Best Paper Honorable Mention Award</span> in Siggraph Asia 2024.</li>
                  <li style="line-height:30px"> <b>2024.03:</b> We release <a href="https://github.com/VAST-AI-Research/TripoSR">TripoSR</a> with Stability AI, which was the fastest image-to-3D reconstruction model at the time.</li>
            		  <li style="line-height:30px"> <b>2023.12:</b> We launches 3D generation product <a href="https://www.tripo3d.ai/">TripoAI</a>.</li>
            		  <!-- <li style="line-height:30px"> <b>November 2023:</b> Release 3DAIGC related papers: <a href="https://yg256li.github.io/UniDream/">UniDream</a>, <a href="https://huanngzh.github.io/EpiDiff/">EpiDiff</a>, <a href="https://zouzx.github.io/TriplaneGaussian/">TGS</a>, <a href="https://xinyu-andy.github.io/Classifier-Score-Distillation/">CSD</a>.</li> -->
                  <!-- <li style="line-height:30px"> <b>October 2022:</b> One paper gets accepted to EMNLP 2022.</li>
                  <li style="line-height:30px"> <b>September 2022:</b> One paper gets accepted to BMVC 2022.</li>
                  <li style="line-height:30px"> <b>September 2022:</b> One paper gets accepted to NeurIPS 2022.</li>
                  <li style="line-height:30px"> <b>August 2022:</b> One paper gets accepted to COLING 2022.</li> -->
                  <!-- <li style="line-height:30px"> <b>August 2022:</b> <a href="https://github.com/enyac-group/supmae">SupMAE</a> code is available.</li> -->
                  <!-- <li style="line-height:30px"> <b>July 2022:</b> <a href="https://github.com/Sense-GVT/BCDNet">BCDNet</a> code is available.</li> -->
<!--                   <li style="line-height:30px"> <b>July 2022:</b> One paper gets accepted to ECCV 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> 1st place in Embodied AI Challenge @ CVPR 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> 2nd place in UG2+ Challenge @ CVPR 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> One papers gets accepted to ICML workshops 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> One papers gets accepted to CVPR workshops 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> One paper gets accepted to IJCAI 2022 as <b>long oral</b>.</li> -->
                  <!-- <li style="line-height:30px"> <b>February 2022:</b> <a href="https://github.com/Sense-GVT/SNCSE">SNCSE</a> code is available.</li> -->
                  <!-- <li style="line-height:30px"> <b>January 2022:</b> SenseTime Team Award, SenseTimeâ€™s highest award.</li> -->
                  <!-- <li style="line-height:30px"> <b>January 2022:</b> <a href="https://github.com/Sense-GVT/DeCLIP">DeCLIP & DeCLIP benchmark</a> code are available.</li> -->
                  <!-- <li style="line-height:30px"> <b>January 2022:</b> One paper gets accepted to ICLR 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> <a href="https://opengvlab.shlab.org.cn/home">OpenGVLab</a> is released.</li>
                  <li style="line-height:30px"> <b>November 2021:</b> <a href="https://arxiv.org/abs/2111.08687">INTERN</a> general vision technical report is released.</li> -->
                <ul>
		    </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected 3D GenAI Papers:</heading>
                <ul>

                  <li style="line-height:30px"> <b>TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models</b> 
                    <br><strong>Yangguang Li</strong>, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao
                    <br>[<a href="https://arxiv.org/abs/2502.06608">Paper</a>][<a href="https://github.com/VAST-AI-Research/TripoSG">Code</a>][Arxiv 2025.2]</li>              

                  <li style="line-height:30px"> <b>TripoSR: Fast 3D Object Reconstruction from a Single Image</b> 
                    <br>Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, <strong>Yangguang Li</strong>, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei Cao
                    <br>[<a href="https://arxiv.org/abs/2403.02151">Paper</a>][<a href="https://github.com/VAST-AI-Research/TripoSR">Code</a>][Arxiv 2024.3]</li>  

                  <li style="line-height:30px"> <b>ShapeGen: Towards High-Quality 3D Shape Synthesis</b> 
                    <br><strong>Yangguang Li</strong>, Xianglong He, Zi-Xin Zou, Zexiang Liu, Wanli Ouyang, Ding Liang, Yan-Pei Cao
                    <br>[Paper][Code][<span style="color:red;">SIGGRAPH Asia 2025]</li>  

                  <li style="line-height:30px"> <b>SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</b> 
                    <br>Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, <strong>Yangguang Li</strong>
                    <br>[<a href="https://arxiv.org/abs/2503.21732">Paper</a>][<a href="https://github.com/VAST-AI-Research/TripoSF">Code</a>][<span style="color:red;"></span>ICCV 2025]</span></li> 

                  <li style="line-height:30px"> <b>TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction</b> 
                    <br>Xuying Zhang, Yutong Liu, <strong>Yangguang Li</strong>, Renrui Zhang, Yufei Liu, Kai Wang, Wanli Ouyang, Zhiwei Xiong, Peng Gao, Qibin Hou, Ming-Ming Cheng
                    <br>[<a href="https://arxiv.org/abs/2412.16919?">Paper</a>][<a href="https://github.com/HVision-NKU/TAR3D">Code</a>][<span style="color:red;">ICCV 2025]</span></li>

                  <li style="line-height:30px"> <b>Dreamcraft3d++: Efficient hierarchical 3d generation with multi-plane reconstruction model</b> 
                    <br>Jingxiang Sun, Cheng Peng, Ruizhi Shao, Yuan-Chen Guo, Xiaochen Zhao, <strong>Yangguang Li</strong>, Yanpei Cao, Bo Zhang, Yebin Liu
                    <br>[<a href="https://arxiv.org/abs/2410.12928">Paper</a>][<a href="https://github.com/deepseek-ai/DreamCraft3D">Code</a>][<span style="color:red;">TPAMI 2025]</span></li>

                  <li style="line-height:30px"> <b>MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation</b> 
                    <br>Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, <strong>Yangguang Li</strong>, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, Lu Sheng
                    <br>[<a href="https://arxiv.org/abs/2412.03558">Paper</a>][<a href="https://github.com/VAST-AI-Research/MIDI-3D">Code</a>][<span style="color:red;">CVPR 2025]</span></li>  

                  <li style="line-height:30px"> <b>PSHuman: Photorealistic Single-image 3D Human Reconstruction using Cross-Scale Multiview Diffusion and Explicit Remeshing</b> 
                    <br>Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, <strong>Yangguang Li</strong>, Xingqun Qi, Xiaowei Chi, Siyu Xia, Yan-Pei Cao, Wei Xue, Wenhan Luo, Yike Guo
                    <br>[<a href="https://arxiv.org/abs/2409.10141">Paper</a>][<a href="https://github.com/pengHTYX/PSHuman">Code</a>][<span style="color:red;">CVPR 2025]</span></li>  

                  <li style="line-height:30px"> <b>TEXGen: a Generative Diffusion Model for Mesh Textures</b> 
                    <br>Xin Yu, Ze Yuan, Yuan-Chen Guo, Ying-Tian Liu, Jianhui Liu, <strong>Yangguang Li</strong>, Yan-Pei Cao, Ding Liang, Xiaojuan Qi
                    <br>[<a href="https://arxiv.org/abs/2411.14740">Paper</a>][<a href="https://github.com/CVMI-Lab/TEXGen">Code</a>][<span style="color:red;">TOG 2024]</span></li>                      

                  <li style="line-height:30px"> <b>Tripo Doodle: The Next-Gen AI 3D Creative Tool</b> 
                    <br>Sienna Hwang, Muqing Jia, Yan-Pei Cao, Yuan-Chen Guo, <strong>Yangguang Li</strong>, Ding Liang
                    <br>[<a href="https://dl.acm.org/doi/abs/10.1145/3681757.3697052">Paper</a>][<a href="https://github.com/VAST-AI-Research/TripoSG">Code</a>][<span style="color:red;">SIGGRAPH Asia 2024 Real-Time Live]</span></li>  

                  <li style="line-height:30px"> <b>Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT</b> 
                    <br>Le Zhuo, Ruoyi Du, Han Xiao, <strong>Yangguang Li</strong>, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao, Hongsheng Li, Peng Gao
                    <br>[<a href="https://arxiv.org/abs/2406.18583">Paper</a>][<a href="https://github.com/Alpha-VLLM/Lumina-T2X">Code</a>][<span style="color:red;">NeurIPS 2024]</span></li>  

                  <li style="line-height:30px"> <b>Fast-BEV: A Fast and Strong Birdâ€™s-Eye View Perception Baseline</b> 
                    <br><strong>Yangguang Li</strong>, Bin Huang, Zeren Chen, Yufeng Cui, Feng Liang, Mingzhu Shen, Fenggang Liu, Enze Xie, Lu Sheng, Wanli Ouyang, Jing Shao
                    <br>[<a href="https://arxiv.org/abs/2301.12511">Paper</a>][<a href="https://github.com/Sense-GVT/Fast-BEV">Code</a>][<span style="color:red;">TPAMI 2024]</span></li>  

                  <li style="line-height:30px"> <b>GVGEN: Text-to-3D Generation with Volumetric Representation</b> 
                    <br>Xianglong He, Junyi Chen, Sida Peng, Di Huang, <strong>Yangguang Li</strong>, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, Tong He
                    <br>[<a href="https://arxiv.org/abs/2403.12957">Paper</a>][<a href="https://github.com/SOTAMak1r/GVGEN">Code</a>][<span style="color:red;">ECCV 2024</span>]</li>

                  <li style="line-height:30px"> <b>UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation</b> 
                    <br>Zexiang Liu, <strong>Yangguang Li</strong>, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, Wanli Ouyang
                    <br>[<a href="https://arxiv.org/abs/2312.08754">Paper</a>][<a href="https://yg256li.github.io/UniDream/">Page</a>][<span style="color:red;">ECCV 2024</span>]</li>
                
                  <li style="line-height:30px"> <b>Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</b> 
                    <br>Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, <strong>Yangguang Li</strong>, Ding Liang, Yan-Pei Cao, Song-Hai Zhang
                    <br>[<a href="https://arxiv.org/abs/2312.09147">Paper</a>][<a href="https://github.com/VAST-AI-Research/TriplaneGaussian">Code</a>][<span style="color:red;">CVPR 2024</span>]</li>  

                  <li style="line-height:30px"> <b>EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion</b> 
                    <br>Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, <strong>Yangguang Li</strong>, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, Lu Sheng
                    <br>[<a href="https://arxiv.org/abs/2312.06725">Paper</a>][<a href="https://github.com/huanngzh/EpiDiff">Code</a>][<span style="color:red;">CVPR 2024</span>]</li>  

                  <li style="line-height:30px"> <b>Text-to-3D with Classifier Score Distillation</b> 
                    <br>Xin Yu, Yuan-Chen Guo, <strong>Yangguang Li</strong>, Ding Liang, Song-Hai Zhang, Xiaojuan Qi
                    <br>[<a href="https://arxiv.org/abs/2310.19415">Paper</a>][<a href="https://github.com/CVMI-Lab/Classifier-Score-Distillation">Code</a>][<span style="color:red;">ICLR 2024</span>]</li>  

                  <li style="line-height:30px"> <b>BEVBert: Multimodal Map Pre-training for Language-guided Navigation</b> 
                    <br>ong An, Yuankai Qi, <strong>Yangguang Li</strong>, Yan Huang, Liang Wang, Tieniu Tan, Jing Shao
                    <br>[<a href="https://arxiv.org/abs/2212.04385">Paper</a>][<a href="https://github.com/MarSaKi/VLN-BEVBert">Code</a>][<span style="color:red;">ICCV 2023</span>]</li>  

                  <li style="line-height:30px"> <b>A Mixture of Surprises for Unsupervised Reinforcement Learning</b> 
                    <br>Andrew Zhao, Matthieu Gaetan Lin, <strong>Yangguang Li</strong>, Yong-Jin Liu, Gao Huang
                    <br>[<a href="https://arxiv.org/abs/2210.06702">Paper</a>][<a href="https://github.com/LeapLabTHU/MOSS">Code</a>][<span style="color:red;">NeurIPS 2022</span>]</li>  

                  <li style="line-height:30px"> <b>Towards Accurate Binary Neural Networks via Modeling Contextual Dependencies</b> 
                    <br>Xingrun Xing, <strong>Yangguang Li</strong>, Wei Li, Wenrui Ding, Yalong Jiang, Yufeng Wang, Jing Shao, Chunlei Liu, Xianglong Liu
                    <br>[<a href="https://arxiv.org/abs/2209.01404">Paper</a>][<a href="https://github.com/Sense-GVT/BCDNet">Code</a>][<span style="color:red;">ECCV 2022</span>]</li>  

                  <li style="line-height:30px"> <b>Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</b> 
                    <br><strong>Yangguang Li</strong>, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, Junjie Yan
                    <br>[<a href="https://arxiv.org/abs/2110.05208">Paper</a>][<a href="https://github.com/Sense-GVT/DeCLIP">Code</a>][<span style="color:red;">ICLR 2022</span>]</li>  

              
                <ul>
		    </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Service:</heading>
                <ul>
                  <li style="line-height:30px">  Serve as a reviewer in CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, etc.</li>
                  <li style="line-height:30px">  ECCV 2022: Workshop Organizers @ Computer Vision in the Wild.</li>
                <ul>
		    </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Award and Honor:</heading>
                <ul>
                  <li style="line-height:30px">  <b>2024.12:</b> Best Paper Honorable Mention Award @ SIGGRAPH Asia 2024</li>
                  <li style="line-height:30px">  <b>2024.01:</b> Best Poster Award @ AAAI 2024 Edge Intelligence Workshop</li>
                  <li style="line-height:30px">  <b>2023.01:</b> SenseTime Team Award, SenseTime's highest award @2022 Autonomous Driving Mass Production projects</li>
                  <li style="line-height:30px">  <b>2022.06:</b> 1st place in Embodied AI Workshop @ CVPR 2022</li>
                  <li style="line-height:30px">  <b>2022.06:</b> 2nd place in UG2+ Challenge @ CVPR 2022</li>  
                  <li style="line-height:30px">  <b>2022.01:</b> SenseTime Team Award, SenseTime's highest award @2021 General Vision Big Models Technology System</li>
                  <li style="line-height:30px">  <b>2020.12:</b> Outstanding Intern @SenseTime2020</li>
                  <li style="line-height:30px">  <b>2019.12:</b> Outstanding Intern @SenseTime2019</li>
                  <li style="line-height:30px">  <b>2019.12:</b> 1st place in Celebrity Video Identification Challenge @ACMMM 2019</li>
                  

                <ul>
		    </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bcdnet.png" alt="bcdnet" width="280" height="100" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2209.01404">
                <papertitle>Towards Accurate Binary Neural Networks via Modeling Contextual Dependencies</papertitle>
              </a>
              <br>
              <a href="">Xingrun Xing</a>,
              <strong>Yangguang Li</strong>,
              <a href="https://weivision.github.io/">Wei Li</a>,
              <a href="">Wenrui Ding</a>,
              <a href="">Yalong Jiang</a>,
              <a href="">Yufeng Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>,
              <a href="">Chunlei Liu</a>,
              <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2209.01404.pdf">PDF</a>,
              <a href="data/bcdnet.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/BCDNet">code</a>
              <p></p>
              <p>We propose BCDNet, a newly designed method to binary neural modules, which enables BNNs to learn effective contextual dependencies.</p>
            </td>
          </tr> -->


<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/declip.png" alt="declip" width="240" height="170" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2110.05208">
                <papertitle>Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</papertitle>
              </a>
              <br>
              <strong>Yangguang Li*</strong>,
              <a href="https://jeff-liangf.github.io/">Feng Liang*</a>,
              <a href="https://openreview.net/profile?id=~Lichen_Zhao1">Lichen Zhao*</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=5Ydha2EAAAAJ">Yufeng Cui</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>,
              <a href="https://forwil.xyz/">Fengwei Yu</a>,
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>
              <br>
              <em>ICLR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2110.05208.pdf">PDF</a>,
              <a href="data/declip.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/DeCLIP">code</a>,
              <a href="https://recorder-v3.slideslive.com/#/share?share=62378&s=d93b81b1-d9de-42b7-9437-8acc34fbf94e">video presentation</a>
              <p></p>
              <p>We propose Data efficient CLIP (DeCLIP), a method to efficiently train CLIP via utilizing the widespread supervision among the image-text data.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/declip_benchmark.png" alt="declip_benchmark" width="260" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.05796">
                <papertitle>Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=5Ydha2EAAAAJ">Yufeng Cui*</a>,
              <a href="https://openreview.net/profile?id=~Lichen_Zhao1">Lichen Zhao*</a>,
              <a href="https://jeff-liangf.github.io/">Feng Liang*</a>,
              <strong>Yangguang Li<sup>&dagger;</sup></strong>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>ICMLW</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.05796.pdf">PDF</a>,
              <a href="data/declip_benchmark.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/DeCLIP">code</a>
              <p></p>
              <p>We propose a CLIP benchmark of data, model, and supervision.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/repre.png" alt="repre" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2201.06857">
                <papertitle>RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training</papertitle>
              </a>
              <br>
              <a href="">Luya Wang</a>,
              <a href="https://jeff-liangf.github.io/">Feng Liang</a>,
              <strong>Yangguang Li</strong>,
              <a href="">Honggang Zhang</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>IJCAI</em>, 2022, <b>Long oral</b>
              <br>
              <a href="https://arxiv.org/pdf/2201.06857.pdf">PDF</a>,
              <a href="data/repre.bib">bibtex</a>
              <p></p>
              <p>We propose RePre to extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective.</p>
            </td>
          </tr> -->



        <!-- </tbody></table> -->

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody> -->
          <!-- <tr> -->
            <!-- <td> -->
              <!-- <heading>Technical Report</heading> -->
            <!-- </td> -->
          <!-- </tr> -->
        <!-- </tbody></table> -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tbd.png" alt="tbd" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.03006">
                <papertitle>Task-Balanced Distillation for Object Detection</papertitle>
              </a>
              <br>
              <a href="">Ruining Tang</a>,
              <a href="">Zhenyu Liu</a>,
              <strong>Yangguang Li</strong>,
              <a href="">Yiguo Song</a>,
              <a href="">Hui Liu</a>,
              <a href="">Qide Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>,
              <a href="">Guifang Duan</a>,
              <a href="">Jianrong Tan</a>
              <br>
              <em>Pattern Recognition</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2208.03006.pdf">PDF</a>,
              <a href="data/tbd.bib">bibtex</a>
              <p></p>
              <p>We alleviate the problem of inconsistent spatial distributions between classification score and localization quality (IOU) in detection task by designing a customized knowledge distillation teacher-student training workflow.</p>
            </td>
          </tr> -->
<!-- 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rxr.png" alt="rxr" width="260" height="170" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2206.11610">
                <papertitle>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=8Zdbbz0AAAAJ&hl=zh-CN">Dong An*</a>,
              <a href="">Zun Wang*</a>,
              <strong>Yangguang Li</strong>,
              <a href="https://shepnerd.github.io/">Yi Wang</a>,
              <a href="https://yiconghong.me/">Yicong Hong</a>,
              <a href="https://yanrockhuang.github.io/">Yan Huang</a>,
              <a href="https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN">Liang Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>Winner of the 2nd RxR-Habitat Competition</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2206.11610.pdf">PDF</a>,
              <a href="data/rxr.bib">bibtex</a>
              <p></p>
              <p>We present a modular plan-and-control approach, which consists of three modules: the candidate waypoints predictor (CWP), the history enhanced planner and the tryout controller.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/imci.png" alt="imci" width="280" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.14001">
                <papertitle>IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification</papertitle>
              </a>
              <br>
              <a href="">Hao Wang</a>,
              <strong>Yangguang Li</strong>,
              <a href="https://scholar.google.com/citations?user=lng9zHEAAAAJ&hl=zh-CN">Zhen Huang</a>,
              <a href="">Yong Dou</a>
              <br>
              <em>COLING</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2208.14001.pdf">PDF</a>,
              <a href="data/imci.bib">bibtex</a>, 
              <a href="https://github.com/phoenixsecularbird/IMCI">code</a>
              <p></p>
              <p>We propose IMCI to integrate multi-view contextual information for fact extraction and verification.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/r2f.png" alt="r2f" width="280" height="90" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>R2F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference</papertitle>
              </a>
              <br>
              <a href="">Hao Wang</a>,
              <a href="https://sites.google.com/view/yixin-homepage">Yixin Cao<sup>&dagger;</sup></a>,
              <strong>Yangguang Li</strong>,
              <a href="https://scholar.google.com/citations?user=lng9zHEAAAAJ&hl=zh-CN">Zhen Huang</a>,
              <a href="">Kun Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>EMNLP</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2210.12328">PDF</a>,
              <a href="data/r2f.bib">bibtex</a>,
              <a href="https://github.com/phoenixsecularbird/R2F">code</a>
              <p></p>
              <p> We establish a general solution, named Retrieval, Reading and Fusion (R2F) framework, and a new setting, by analyzing the main challenges of DocNLI: interpretability, long-range dependency, and cross-sentence inference.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/intern.png" alt="intern" width="280" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2111.08687">
                <papertitle>INTERN: A New Learning Paradigm Towards General Vision</papertitle>
              </a>
              <br>
              <a href="https://amandajshao.github.io/">Jing Shao*</a>,
              <a href="https://scholar.google.pl/citations?user=Ckmfbo8AAAAJ&hl=fr">Siyu Chen*</a>,
              <strong>Yangguang Li*</strong>,
              <a href="">Kun Wang*</a>,
              <a href="">Zhenfei Yin*</a>,
              <a href="">Yinan He*</a>,
              <a href="">Jianing Teng*</a>,
              <a href="">Qinghong Sun*</a>,
              <a href="">Mengya Gao*</a>,
              <a href="">Jihao Liu*</a>,
              <a href="">Gengshi Huang*</a>,
              .etc
              <br>
              <em>CoRR</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2111.08687.pdf">PDF</a>,
              <a href="data/intern.bib">bibtex</a>,
              <a href="https://github.com/OpenGVLab/gv-benchmark">code</a>,
              <a href="https://opengvlab.shlab.org.cn/home">project</a>

              <p></p>
              <p>we propose a new learning paradigm named INTERN, which introduces a continuous learning scheme, including a highly extensible upstream pretraining pipeline leveraging large-scale data and various supervisory signals, as well as a flexible downstream adaptation towards diversified tasks.</p>
            </td>
          </tr> -->
<!--         </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Honors</heading>
                <ul>
                  <li style="line-height:30px"> SenseTime Team Award, SenseTime's highest award, 2021.</li>
                  <li style="line-height:30px"> SenseTime Outstanding Intern, 2020.</li>
                  <li style="line-height:30px"> SenseTime Outstanding Intern, 2019.</li>
            </td>
          </tr>
        </tbody></table> -->


<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
                <p>
                  <li style="line-height:30px"> Reviewer in CVPR, ICML, ECCV, NeurIPS, TCSVT</li>
                  <li style="line-height:30px"> Organizer in ECCV 2022 Computer Vision in the Wild Challenge</li>
                </p>
            </td>
          </tr>
        </tbody></table> -->


<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                <a href="https://clustrmaps.com/site/1bqdz"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=XGFQwNh_7Kz54sbp6qL_1YSxklUHJ1TQvZdqIhX0S94&cl=ffffff" /></a> -->
<!--               <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/Jeff-LiangF/jeff-liangf.github.io">Feng (Jeff) Liang</a>
              </p> -->
<!--             </td>
          </tr>
        </tbody></table> -->
      </td>
    </tr>
  </table>
</body>

</html>
