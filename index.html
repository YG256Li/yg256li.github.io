<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yangguang Li</title>

  <meta name="author" content="Yangguang Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/ut_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">

            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/lyg.png"><img style="width:85%;max-width:85%" alt="profile photo" src="images/lyg.png" class="hoverZoomLink"></a>
            </td>
            
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yangguang Li </name>
              </p>
              <p style="text-align:center">
              Now, I am a Research Director at VAST from 2023. 
              </p>

              <p style="text-align:center">
              Before that I was a Research Leader at SenseTime</a>. 
              <!-- I obtained my master degree from <a href="https://www.bupt.edu.cn">Beijing University of Posts and Telecommunications</a> and bachelor degree from <a href="http://www.tju.edu.cn">Tianjin University</a>. -->
              </p>

              <p style="text-align:center">
              Currently, I am focusing on 3D generation research.
              </p>
              <p style="text-align:center">
                <p style="text-align:center">
                  I am PhD in the Chinese University of Hong Kong.
                  </p>
                  <p style="text-align:center">
                <a href="mailto:liyangguang256@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV_JeffLiang.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?user=ecTFCUMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                <!-- <a href="https://www.linkedin.com/in/yg-li-237683223/">Linkedin</a> &nbsp/&nbsp -->
                <!-- <a href="https://www.zhihu.com/people/atom-74">Zhihu</a> &nbsp/&nbsp -->
                <!-- <a href="https://github.com/Sense-GVT">Github</a> &nbsp/&nbsp -->
		            <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=a7AMvgkAAAAJ">Google Scholar</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
                <ul>
                  <li style="line-height:30px"> <b>2025.03:</b> We release <a href="https://github.com/VAST-AI-Research/TripoSF">TripoSF</a>, which was the best open-source image-to-3D generation model.</li>
                  <li style="line-height:30px"> <b>2025.02:</b> We release <a href="https://github.com/VAST-AI-Research/TripoSR">TripoSG</a>, which was the best open-source image-to-3D generation model at the time.</li>
                  <li style="line-height:30px"> <b>2024.12:</b> Our paper <a href="https://arxiv.org/abs/2411.14740">TexGen</a> won the Best Paper Honorable Mention award in Siggraph Asia 2024.</li>
                  <li style="line-height:30px"> <b>2024.03:</b> We release <a href="https://github.com/VAST-AI-Research/TripoSR">TripoSR</a> with Stability AI, which was the fastest open-source image-to-3D reconstruction model at the time.</li>
            		  <li style="line-height:30px"> <b>2023.12:</b> We launches 3D generation product <a href="https://www.tripo3d.ai/">TripoAI</a>.</li>
            		  <!-- <li style="line-height:30px"> <b>November 2023:</b> Release 3DAIGC related papers: <a href="https://yg256li.github.io/UniDream/">UniDream</a>, <a href="https://huanngzh.github.io/EpiDiff/">EpiDiff</a>, <a href="https://zouzx.github.io/TriplaneGaussian/">TGS</a>, <a href="https://xinyu-andy.github.io/Classifier-Score-Distillation/">CSD</a>.</li> -->
                  <!-- <li style="line-height:30px"> <b>October 2022:</b> One paper gets accepted to EMNLP 2022.</li>
                  <li style="line-height:30px"> <b>September 2022:</b> One paper gets accepted to BMVC 2022.</li>
                  <li style="line-height:30px"> <b>September 2022:</b> One paper gets accepted to NeurIPS 2022.</li>
                  <li style="line-height:30px"> <b>August 2022:</b> One paper gets accepted to COLING 2022.</li> -->
                  <!-- <li style="line-height:30px"> <b>August 2022:</b> <a href="https://github.com/enyac-group/supmae">SupMAE</a> code is available.</li> -->
                  <!-- <li style="line-height:30px"> <b>July 2022:</b> <a href="https://github.com/Sense-GVT/BCDNet">BCDNet</a> code is available.</li> -->
<!--                   <li style="line-height:30px"> <b>July 2022:</b> One paper gets accepted to ECCV 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> 1st place in Embodied AI Challenge @ CVPR 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> 2nd place in UG2+ Challenge @ CVPR 2022.</li>
                  <li style="line-height:30px"> <b>June 2022:</b> One papers gets accepted to ICML workshops 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> One papers gets accepted to CVPR workshops 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> One paper gets accepted to IJCAI 2022 as <b>long oral</b>.</li> -->
                  <!-- <li style="line-height:30px"> <b>February 2022:</b> <a href="https://github.com/Sense-GVT/SNCSE">SNCSE</a> code is available.</li> -->
                  <!-- <li style="line-height:30px"> <b>January 2022:</b> SenseTime Team Award, SenseTimeâ€™s highest award.</li> -->
                  <!-- <li style="line-height:30px"> <b>January 2022:</b> <a href="https://github.com/Sense-GVT/DeCLIP">DeCLIP & DeCLIP benchmark</a> code are available.</li> -->
                  <!-- <li style="line-height:30px"> <b>January 2022:</b> One paper gets accepted to ICLR 2022.</li>
                  <li style="line-height:30px"> <b>April 2022:</b> <a href="https://opengvlab.shlab.org.cn/home">OpenGVLab</a> is released.</li>
                  <li style="line-height:30px"> <b>November 2021:</b> <a href="https://arxiv.org/abs/2111.08687">INTERN</a> general vision technical report is released.</li> -->
                <ul>
		    </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Recent Papers</heading>
                <ul>
                  <li style="line-height:30px"> <b>(TripoSF) SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</b> 
                    <br>Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, <strong>Yangguang Li</strong>
                    <br>[<a href="https://github.com/VAST-AI-Research/TripoSF">Paper</a>][<a href="https://github.com/VAST-AI-Research/TripoSF">Code</a>][Arxiv 2025.3]</li>              

                  <li style="line-height:30px"> <b>TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models</b> 
                    <br><strong>Yangguang Li</strong>, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao
                    <br>[<a href="https://arxiv.org/abs/2502.06608">Paper</a>][<a href="https://github.com/VAST-AI-Research/TripoSG">Code</a>][Arxiv 2025.2]</li>              

                  <li style="line-height:30px"> <b>TripoSR: Fast 3D Object Reconstruction from a Single Image</b> 
                    <br>Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, <strong>Yangguang Li</strong>, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei Cao
                    <br>[<a href="https://arxiv.org/abs/2403.02151">Paper</a>][<a href="https://github.com/VAST-AI-Research/TripoSR">Code</a>][Arxiv 2024.3]</li>  
          
                  <li style="line-height:30px"> <b>MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation</b> 
                    <br>Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, <strong>Yangguang Li</strong>, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, Lu Sheng
                    <br>[<a href="https://arxiv.org/abs/2412.03558">Paper</a>][<a href="https://github.com/VAST-AI-Research/MIDI-3D">Code</a>][CVPR 2025]</li>  

                  <li style="line-height:30px"> <b>PSHuman: Photorealistic Single-image 3D Human Reconstruction using Cross-Scale Multiview Diffusion and Explicit Remeshing</b> 
                    <br>Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, <strong>Yangguang Li</strong>, Xingqun Qi, Xiaowei Chi, Siyu Xia, Yan-Pei Cao, Wei Xue, Wenhan Luo, Yike Guo
                    <br>[<a href="https://arxiv.org/abs/2412.03558">Paper</a>][<a href="https://github.com/pengHTYX/PSHuman">Code</a>][CVPR 2025]</li>  


                <ul>
		    </td>
          </tr>
        </tbody></table>



<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bcdnet.png" alt="bcdnet" width="280" height="100" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2209.01404">
                <papertitle>Towards Accurate Binary Neural Networks via Modeling Contextual Dependencies</papertitle>
              </a>
              <br>
              <a href="">Xingrun Xing</a>,
              <strong>Yangguang Li</strong>,
              <a href="https://weivision.github.io/">Wei Li</a>,
              <a href="">Wenrui Ding</a>,
              <a href="">Yalong Jiang</a>,
              <a href="">Yufeng Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>,
              <a href="">Chunlei Liu</a>,
              <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2209.01404.pdf">PDF</a>,
              <a href="data/bcdnet.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/BCDNet">code</a>
              <p></p>
              <p>We propose BCDNet, a newly designed method to binary neural modules, which enables BNNs to learn effective contextual dependencies.</p>
            </td>
          </tr> -->


<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/declip.png" alt="declip" width="240" height="170" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2110.05208">
                <papertitle>Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</papertitle>
              </a>
              <br>
              <strong>Yangguang Li*</strong>,
              <a href="https://jeff-liangf.github.io/">Feng Liang*</a>,
              <a href="https://openreview.net/profile?id=~Lichen_Zhao1">Lichen Zhao*</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=5Ydha2EAAAAJ">Yufeng Cui</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>,
              <a href="https://forwil.xyz/">Fengwei Yu</a>,
              <a href="https://yan-junjie.github.io/">Junjie Yan</a>
              <br>
              <em>ICLR</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2110.05208.pdf">PDF</a>,
              <a href="data/declip.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/DeCLIP">code</a>,
              <a href="https://recorder-v3.slideslive.com/#/share?share=62378&s=d93b81b1-d9de-42b7-9437-8acc34fbf94e">video presentation</a>
              <p></p>
              <p>We propose Data efficient CLIP (DeCLIP), a method to efficiently train CLIP via utilizing the widespread supervision among the image-text data.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/declip_benchmark.png" alt="declip_benchmark" width="260" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.05796">
                <papertitle>Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=5Ydha2EAAAAJ">Yufeng Cui*</a>,
              <a href="https://openreview.net/profile?id=~Lichen_Zhao1">Lichen Zhao*</a>,
              <a href="https://jeff-liangf.github.io/">Feng Liang*</a>,
              <strong>Yangguang Li<sup>&dagger;</sup></strong>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>ICMLW</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.05796.pdf">PDF</a>,
              <a href="data/declip_benchmark.bib">bibtex</a>,
              <a href="https://github.com/Sense-GVT/DeCLIP">code</a>
              <p></p>
              <p>We propose a CLIP benchmark of data, model, and supervision.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/repre.png" alt="repre" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2201.06857">
                <papertitle>RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training</papertitle>
              </a>
              <br>
              <a href="">Luya Wang</a>,
              <a href="https://jeff-liangf.github.io/">Feng Liang</a>,
              <strong>Yangguang Li</strong>,
              <a href="">Honggang Zhang</a>,
              <a href="https://wlouyang.github.io/">Wanli Ouyang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>IJCAI</em>, 2022, <b>Long oral</b>
              <br>
              <a href="https://arxiv.org/pdf/2201.06857.pdf">PDF</a>,
              <a href="data/repre.bib">bibtex</a>
              <p></p>
              <p>We propose RePre to extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective.</p>
            </td>
          </tr> -->



        <!-- </tbody></table> -->

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody> -->
          <!-- <tr> -->
            <!-- <td> -->
              <!-- <heading>Technical Report</heading> -->
            <!-- </td> -->
          <!-- </tr> -->
        <!-- </tbody></table> -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tbd.png" alt="tbd" width="280" height="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.03006">
                <papertitle>Task-Balanced Distillation for Object Detection</papertitle>
              </a>
              <br>
              <a href="">Ruining Tang</a>,
              <a href="">Zhenyu Liu</a>,
              <strong>Yangguang Li</strong>,
              <a href="">Yiguo Song</a>,
              <a href="">Hui Liu</a>,
              <a href="">Qide Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>,
              <a href="">Guifang Duan</a>,
              <a href="">Jianrong Tan</a>
              <br>
              <em>Pattern Recognition</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2208.03006.pdf">PDF</a>,
              <a href="data/tbd.bib">bibtex</a>
              <p></p>
              <p>We alleviate the problem of inconsistent spatial distributions between classification score and localization quality (IOU) in detection task by designing a customized knowledge distillation teacher-student training workflow.</p>
            </td>
          </tr> -->
<!-- 
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rxr.png" alt="rxr" width="260" height="170" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2206.11610">
                <papertitle>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=8Zdbbz0AAAAJ&hl=zh-CN">Dong An*</a>,
              <a href="">Zun Wang*</a>,
              <strong>Yangguang Li</strong>,
              <a href="https://shepnerd.github.io/">Yi Wang</a>,
              <a href="https://yiconghong.me/">Yicong Hong</a>,
              <a href="https://yanrockhuang.github.io/">Yan Huang</a>,
              <a href="https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN">Liang Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>Winner of the 2nd RxR-Habitat Competition</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2206.11610.pdf">PDF</a>,
              <a href="data/rxr.bib">bibtex</a>
              <p></p>
              <p>We present a modular plan-and-control approach, which consists of three modules: the candidate waypoints predictor (CWP), the history enhanced planner and the tryout controller.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/imci.png" alt="imci" width="280" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2208.14001">
                <papertitle>IMCI: Integrate Multi-view Contextual Information for Fact Extraction and Verification</papertitle>
              </a>
              <br>
              <a href="">Hao Wang</a>,
              <strong>Yangguang Li</strong>,
              <a href="https://scholar.google.com/citations?user=lng9zHEAAAAJ&hl=zh-CN">Zhen Huang</a>,
              <a href="">Yong Dou</a>
              <br>
              <em>COLING</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2208.14001.pdf">PDF</a>,
              <a href="data/imci.bib">bibtex</a>, 
              <a href="https://github.com/phoenixsecularbird/IMCI">code</a>
              <p></p>
              <p>We propose IMCI to integrate multi-view contextual information for fact extraction and verification.</p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/r2f.png" alt="r2f" width="280" height="90" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>R2F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference</papertitle>
              </a>
              <br>
              <a href="">Hao Wang</a>,
              <a href="https://sites.google.com/view/yixin-homepage">Yixin Cao<sup>&dagger;</sup></a>,
              <strong>Yangguang Li</strong>,
              <a href="https://scholar.google.com/citations?user=lng9zHEAAAAJ&hl=zh-CN">Zhen Huang</a>,
              <a href="">Kun Wang</a>,
              <a href="https://amandajshao.github.io/">Jing Shao</a>
              <br>
              <em>EMNLP</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2210.12328">PDF</a>,
              <a href="data/r2f.bib">bibtex</a>,
              <a href="https://github.com/phoenixsecularbird/R2F">code</a>
              <p></p>
              <p> We establish a general solution, named Retrieval, Reading and Fusion (R2F) framework, and a new setting, by analyzing the main challenges of DocNLI: interpretability, long-range dependency, and cross-sentence inference.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/intern.png" alt="intern" width="280" height="140" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2111.08687">
                <papertitle>INTERN: A New Learning Paradigm Towards General Vision</papertitle>
              </a>
              <br>
              <a href="https://amandajshao.github.io/">Jing Shao*</a>,
              <a href="https://scholar.google.pl/citations?user=Ckmfbo8AAAAJ&hl=fr">Siyu Chen*</a>,
              <strong>Yangguang Li*</strong>,
              <a href="">Kun Wang*</a>,
              <a href="">Zhenfei Yin*</a>,
              <a href="">Yinan He*</a>,
              <a href="">Jianing Teng*</a>,
              <a href="">Qinghong Sun*</a>,
              <a href="">Mengya Gao*</a>,
              <a href="">Jihao Liu*</a>,
              <a href="">Gengshi Huang*</a>,
              .etc
              <br>
              <em>CoRR</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2111.08687.pdf">PDF</a>,
              <a href="data/intern.bib">bibtex</a>,
              <a href="https://github.com/OpenGVLab/gv-benchmark">code</a>,
              <a href="https://opengvlab.shlab.org.cn/home">project</a>

              <p></p>
              <p>we propose a new learning paradigm named INTERN, which introduces a continuous learning scheme, including a highly extensible upstream pretraining pipeline leveraging large-scale data and various supervisory signals, as well as a flexible downstream adaptation towards diversified tasks.</p>
            </td>
          </tr> -->
<!--         </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Honors</heading>
                <ul>
                  <li style="line-height:30px"> SenseTime Team Award, SenseTime's highest award, 2021.</li>
                  <li style="line-height:30px"> SenseTime Outstanding Intern, 2020.</li>
                  <li style="line-height:30px"> SenseTime Outstanding Intern, 2019.</li>
            </td>
          </tr>
        </tbody></table> -->


<!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
                <p>
                  <li style="line-height:30px"> Reviewer in CVPR, ICML, ECCV, NeurIPS, TCSVT</li>
                  <li style="line-height:30px"> Organizer in ECCV 2022 Computer Vision in the Wild Challenge</li>
                </p>
            </td>
          </tr>
        </tbody></table> -->


<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                <a href="https://clustrmaps.com/site/1bqdz"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=XGFQwNh_7Kz54sbp6qL_1YSxklUHJ1TQvZdqIhX0S94&cl=ffffff" /></a> -->
<!--               <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/Jeff-LiangF/jeff-liangf.github.io">Feng (Jeff) Liang</a>
              </p> -->
<!--             </td>
          </tr>
        </tbody></table> -->
      </td>
    </tr>
  </table>
</body>

</html>
